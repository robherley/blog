<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>üè° Homelab I: Intro, Hardware and Proxmox install | rob's blog</title><link rel=icon type=image/png href=https://blog.reb.gg/images/logo.png><meta property="og:url" content="https://blog.reb.gg/posts/01-homelab-pt1/">
<meta property="og:site_name" content="rob's blog"><meta property="og:title" content="üè° Homelab I: Intro, Hardware and Proxmox install"><meta property="og:description" content="üëã Hey y‚Äôall, welcome to the start of my homelab (or, the latest iteration of it). I‚Äôm documenting my homelab setup as a helpful guide for others to learn, detailing some of the annoying issues I encountered and some of the duct tape solutions. These are also backup instructions for when I inevitably scrap this homelab, rebuild, and forget how I did everything.
This series will be centered around the setup and automation of a Promox cluster comprised of three very different machines. The virtualization cluster will house a variety of workloads, including:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-09T00:00:00-05:00"><meta property="article:modified_time" content="2022-01-09T00:00:00-05:00"><meta property="article:tag" content="Proxmox"><meta property="article:tag" content="Zfs"><meta property="article:tag" content="Hardware"><meta name=twitter:card content="summary"><meta name=twitter:title content="üè° Homelab I: Intro, Hardware and Proxmox install"><meta name=twitter:description content="üëã Hey y‚Äôall, welcome to the start of my homelab (or, the latest iteration of it). I‚Äôm documenting my homelab setup as a helpful guide for others to learn, detailing some of the annoying issues I encountered and some of the duct tape solutions. These are also backup instructions for when I inevitably scrap this homelab, rebuild, and forget how I did everything.
This series will be centered around the setup and automation of a Promox cluster comprised of three very different machines. The virtualization cluster will house a variety of workloads, including:"><link rel=stylesheet href=/css/custom.min.faefcaabf97ea44e2b9626162349aeb966c6ea5166b9d744503b93cdd81173ad.css integrity="sha256-+u/Kq/l+pE4rliYWI0muuWbG6lFmuddEUDuTzdgRc60=" crossorigin=anonymous><link rel=stylesheet href=/css/article.min.3d68b337d54382250563c3c74528261d25b316f1df232ec198665d91e6621ba7.css integrity="sha256-PWizN9VDgiUFY8PHRSgmHSWzFvHfIy7BmGZdkeZiG6c=" crossorigin=anonymous><link rel=stylesheet href=/css/fonts.min.e0bf373169e09cf6dfc780f970638fcd39a5898dcecb6c9286f170d5907dbbf5.css integrity="sha256-4L83MWngnPbfx4D5cGOPzTmliY3Oy2yShvFw1ZB9u/U=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.61771936e5acd378a831a0ce3858ec58f08eeb0598159d8d3477988eb62c02ee.css integrity="sha256-YXcZNuWs03ioMaDOOFjsWPCO6wWYFZ2NNHeYjrYsAu4=" crossorigin=anonymous><link rel=stylesheet href=/css/simple-icons.min.min.8e53216f6540542c220ed40839d164fb18b299e5d70551cb60b0600d3f0a011f.css integrity="sha256-jlMhb2VAVCwiDtQIOdFk+xiymeXXBVHLYLBgDT8KAR8=" crossorigin=anonymous><style>:root{--color-primary:var(--color-blue-6)}</style><script src=/js/main.95176a6398aade31e73736833a061caca263c49d5f1e01d192b9e651423e0f8c.js integrity="sha256-lRdqY5iq3jHnNzaDOgYcrKJjxJ1fHgHRkrnmUUI+D4w=" crossorigin=anonymous></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script></head><body><header><h1 class=site-title><span role=img aria-label=emoji-logo>üíæ
</span><a href=https://blog.reb.gg/>rob's blog</a></h1><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Posts</a></li></ul></nav></header><main><article><h1 class=title>üè° Homelab I: Intro, Hardware and Proxmox install</h1><div class=meta><time datetime=2022-01-09T00:00:00-05:00>January 9, 2022</time> &#183; 7 min read</div><div class=tags><a class=tag href=/tags/proxmox/>Proxmox</a>
<a class=tag href=/tags/zfs/>Zfs</a>
<a class=tag href=/tags/hardware/>Hardware</a></div><div class=markdown-body><p>üëã Hey y&rsquo;all, welcome to the start of my homelab (or, the latest iteration of it). I&rsquo;m documenting my homelab setup as a helpful guide for others to learn, detailing some of the annoying issues I encountered and some of the duct tape solutions. These are also backup instructions for when I inevitably scrap this homelab, rebuild, and forget how I did everything.</p><p>This series will be centered around the setup and automation of a Promox cluster comprised of three very different machines. The virtualization cluster will house a variety of workloads, including:</p><ul><li>Kubernetes/OpenShift cluster(s)</li><li>Media servers (<a href=https://jellyfin.org/>Jellyfin</a>)</li><li>Network attached storage (<a href=https://www.truenas.com/truenas-scale/>TrueNAS Scale</a>)</li><li>Adblocker (<a href=https://pi-hole.net/>Pi-hole</a>)</li><li>Proxy servers, web apps, minecraft and more</li></ul><p>The goal is to keep the hosts as appliance-like as possible, and have most actions reproducable via automation. All of the relevant code will be available on <a href=https://github.com/robherley/homelab>robherley/homelab</a>, feel free to contribute.</p><p>In this first part, I&rsquo;ll go over the hardware, some minor hacks, and the initial baremetal Proxmox install.</p><h2 id=hardware><a href=#hardware>#</a>
Hardware</h2><img src=/content/homelab/rack.png alt=Rack><h3 id=dell-poweredge-r720xdhttpswwwdellcomen-usworkshopproductdetailstxnpoweredge-r720xd><a href=#dell-poweredge-r720xdhttpswwwdellcomen-usworkshopproductdetailstxnpoweredge-r720xd>#</a>
<a href=https://www.dell.com/en-us/work/shop/productdetailstxn/poweredge-r720xd>Dell Poweredge r720xd</a></h3><ul><li><strong>CPU:</strong> 2 x Intel Xeon (16) @ 2.50GHz</li><li><strong>Memory:</strong> 16 x Kingston 8GB DDR3-1600</li><li><strong>Storage:</strong><ul><li>1 x SATA Samsung Evo 850 500GB SSD</li><li>2 x SATA Crucial MX500 1TB SSD</li><li>9 x SATA Seagate Constellation 1TB HDD</li><li>2 x SAS Seagate Constellation 1TB HDD</li></ul></li></ul><h3 id=intel-nuci7behhttpswwwintelcomcontentwwwusenproductssku126140intel-nuc-kit-nuc8i7behspecificationshtml><a href=#intel-nuci7behhttpswwwintelcomcontentwwwusenproductssku126140intel-nuc-kit-nuc8i7behspecificationshtml>#</a>
<a href=https://www.intel.com/content/www/us/en/products/sku/126140/intel-nuc-kit-nuc8i7beh/specifications.html>Intel NUCi7BEH</a></h3><ul><li><strong>CPU:</strong> Intel i7-8559U (8) @ 4.50GHz</li><li><strong>Memory:</strong> Mixed 32GB (16x2) DDR4-2666</li><li><strong>Storage:</strong><ul><li>Crucial m.2 1TB SSD</li></ul></li></ul><h3 id=intel-2000-family-jbodhttpswwwintelcomcontentdamsupportusendocumentsserver-productsserver-systemsjbod20hwg_v142pdf><a href=#intel-2000-family-jbodhttpswwwintelcomcontentdamsupportusendocumentsserver-productsserver-systemsjbod20hwg_v142pdf>#</a>
<a href=https://www.intel.com/content/dam/support/us/en/documents/server-products/server-systems/JBOD%20HWG_v.1.42.pdf>Intel 2000 Family JBOD</a></h3><ul><li>Connected to r720xd via Intel LSI HBA.</li><li><strong>Storage:</strong> (Total capacity of 12 drives)<ul><li>4 x Seagate IronWolf 4TB NAS HDD</li></ul></li></ul><h3 id=raspberry-pi-3-model-bhttpswwwraspberrypicomproductsraspberry-pi-3-model-b><a href=#raspberry-pi-3-model-bhttpswwwraspberrypicomproductsraspberry-pi-3-model-b>#</a>
<a href=https://www.raspberrypi.com/products/raspberry-pi-3-model-b/>Raspberry Pi 3 Model B</a></h3><ul><li><strong>CPU:</strong> Quad Core 1.2GHz Broadcom BCM2837</li><li><strong>Memory:</strong> 1GB RAM</li><li><strong>Storage:</strong><ul><li>8GB Micro SDHC</li></ul></li></ul><h3 id=rack-components><a href=#rack-components>#</a>
Rack Components</h3><ul><li><a href=https://www.startech.com/en-us/server-management/4postrack12u>Startech 12U rack</a></li><li><a href=https://www.amazon.com/dp/B00006B83A>Tripp Lite 1U Surge Protector</a></li><li><a href=https://www.amazon.com/dp/B008X3JHJQ>StarTech 2U Shelf</a></li><li><a href=https://www.amazon.com/dp/B0060RUVBA>StartTech 1U Rails</a></li></ul><h3 id=networking><a href=#networking>#</a>
Networking</h3><p>Unfortunately, I do not have dedicated networking hardware yet. I&rsquo;m using the <a href=https://www.verizon.com/home/accessories/fios-router/>Verizon G3100</a> for now. The Raspberry Pi is running <a href=https://pi-hole.net/>Pi-hole</a> for custom DNS and it also handles DHCP.</p><h2 id=hacks><a href=#hacks>#</a>
Hacks</h2><h3 id=flashing-raid-controller><a href=#flashing-raid-controller>#</a>
Flashing RAID controller</h3><p>For my homelab I decided to use ZFS, a software-based RAID. Unfortunately, the included controller with the Poweredge r720xd is expecting to always use a hardware RAID-based virtual disk setup. In order to force the included RAID controller to allow individual use of each disk, it needs to be flashed into IT mode. Fortunately, the folks at <a href=https://fohdeesha.com/docs/perc.html>Fohdeesha</a> have amazing guides for flashing the firmware on all kinds of RAID controllers, including the PERC H710 in this r720xd. After disconnecting the RAID battery, live booting into DOS and Linux images and few shell commands, I was ready to go.</p><h3 id=lsi-and-seagate-headache><a href=#lsi-and-seagate-headache>#</a>
LSI and Seagate headache</h3><p>Luckily, the Intel branded LSI controller used as an HBA can be configured in JBOD mode, so it did not require any flashing to connect the r720xd and the Seagate drives. But, when testing some zpools I noticed a tremendous amount of <code>blk_update_request</code> I/O errors from the kernel. Turns out, there&rsquo;s a spinup/down issue with the Seagate Ironwolf drives and some LSI controllers. Specifically when the disks go into standby mode there are some delays for them to spin up, resulting in the <code>blk_update_request</code> I/O errors I was seeing in the kernel.</p><p>First fix attempt was to force some settings in <a href=https://wiki.archlinux.org/title/Hdparm><code>hdparm</code></a>:</p><ul><li>disabling the standby timeout (<code>hdparm -S 0 &lt;disk></code>)</li><li>disabling the advance power management setting (<code>hdparm -B 255 &lt;disk></code>).</li></ul><p>In addition, Seagate has a special drive utility called SeaChest, I followed <a href=https://forums.unraid.net/topic/103938-69x-lsi-controllers-ironwolf-disks-disabling-summary-fix/>this guide</a>
from a user on the Unraid forum. Within that guide, there&rsquo;ll be instructions to disable EPC and low current spinup, which helped users resolve the drive issues.</p><h3 id=loud-jbod><a href=#loud-jbod>#</a>
Loud JBOD</h3><p>This entire rack is next to my desk, so reducing the amount of noise it makes is pretty important. At idle, the r720xd is suprisingly quiet, whereas the Intel JBOD consistently sounds like a jet engine. So, I grabbed my soldering iron, some wire cutters and three of Noctua&rsquo;s <a href=https://noctua.at/en/nf-a6x25-pwm>NF-A6x25</a>&rsquo;s. It was a relatively painless replacement, and the rubberized feet on the fans gave them a nice friction fit. They have roughly the same size, PWM layout and voltage as the stock Nidec Ultraflo fans. But, they have noticibly less airflow. Fortunately with only four drives in the JBOD they manage to stay nice and cool.</p><img src=/content/homelab/jbod_fans.png alt="JBOD fan replacement"><h3 id=drive-caddies><a href=#drive-caddies>#</a>
Drive caddies</h3><p>I didn&rsquo;t have any extra of the OEM Dell drive caddies for the three SSDs I&rsquo;m installing into the poweredge. Luckily, <a href=https://www.thingiverse.com/thing:2491236/>someone on thingiverse</a> made these really sweet models that resememble mini versions of the poweredge. They are sturdy enough to reliably hold the SSDs in place, but I would not recommend them for spinning rust.</p><img src=/content/homelab/caddies.png alt="3D Printed Caddies"><h2 id=storage-overview><a href=#storage-overview>#</a>
Storage overview</h2><p>For redundancy, most of the homelab storage will be using ZFS. The YouTube channel Level1Techs has an <a href="https://www.youtube.com/watch?v=uBfXdJGmWoM">amazing breakdown</a> on ZFS and software RAID in general.</p><h3 id=nuc><a href=#nuc>#</a>
NUC</h3><h4 id=1tb-ssd---host-os--guest-vms><a href=#1tb-ssd---host-os--guest-vms>#</a>
1TB SSD - Host OS & Guest VMs</h4><p>Since storage options are pretty limited in the NUC, it is just using a single 1TB m.2 SSD shared for both the host OS and guest VMs. Eventually, I would like to squeeze add an additional 1TB SATA ssd for mirror ZFS, but this is fine for now. Backups for guest VMs on this single drive will be on network storage.</p><h3 id=r720><a href=#r720>#</a>
r720</h3><h4 id=500gb-ssd---host-os><a href=#500gb-ssd---host-os>#</a>
500GB SSD - Host OS</h4><p>I&rsquo;m just using a single 500GB SSD for the host proxmox OS. This drive can be even smaller, but it was what I had laying around. Ideally this would also be a mirrored setup but a single drive is fine for now. In the event of this drive failing, I&rsquo;ll just swap it out, reinstall OS, run Ansible and/or restore from backups. For me personally, redundacy for the host OS isn&rsquo;t as critical for the VM guests and network storage.</p><h4 id=ssd-mirror---zfs-mirror-of-2x-1tb-crucial-ssds><a href=#ssd-mirror---zfs-mirror-of-2x-1tb-crucial-ssds>#</a>
<code>ssd-mirror</code> - ZFS mirror of 2x 1TB Crucial SSDs</h4><p>These are the fastest drives I have, so all the guest VM disk images will be on here. It&rsquo;s a mirror, so I can quickly recover from one of these drives failing.</p><h4 id=rusty-z2---zfs-raidz2-of-11x-1tb-seagate-enterprise-constellations><a href=#rusty-z2---zfs-raidz2-of-11x-1tb-seagate-enterprise-constellations>#</a>
<code>rusty-z2</code> - ZFS RAIDz2 of 11x 1TB Seagate enterprise constellations</h4><p>This pool is slower, so it&rsquo;s used for all VM backups from the <code>ssd-mirror</code> pool and the backups from the NUC vm&rsquo;s over NFS. Note the <code>z2</code>, I have two parity drives in this pool, since these drives are a bit older and have more use.</p><h4 id=wolves-z---zfs-raidz-of-4x-4tb-seagate-ironwolf-nas><a href=#wolves-z---zfs-raidz-of-4x-4tb-seagate-ironwolf-nas>#</a>
<code>wolves-z</code> - ZFS RAIDz of 4x 4TB Seagate Ironwolf NAS</h4><p>This&rsquo;ll be primarily for media storage and other network attached storage. In fact, the entire controller for these disks (the HBA connected to JBOD) will be passed through to the TrueNAS guest. These drives are a nice compromise of speed, capacity and cost. Note the <code>z</code>, this pool has a single parity drive.</p><h2 id=proxmox-setup><a href=#proxmox-setup>#</a>
Proxmox setup</h2><h3 id=installation><a href=#installation>#</a>
Installation</h3><p>Proxmox will be installed on both the Poweredge and the NUC. The Pi is going to have a different process since it will only be used for quorum, so it&rsquo;ll be using <a href=https://www.raspberrypi.com/software/>Raspberry Pi OS</a>. The Proxmox installer is pretty straightforward, just write the ISO to a USB, boot into the installer, pick a root disk, setup user credentials and set a static IP. Here are some tutorials by awesome homelab YouTube channels:</p><ul><li>TechnoTim: <a href="https://www.youtube.com/watch?v=7OVaWaqO2aU">Proxmox VE Install and Setup Tutorial</a></li><li>Craft Computing: <a href="https://www.youtube.com/watch?v=azORbxrItOo">Virtualize Everything! - Proxmox Install Tutorial</a></li></ul><p>After installation, the Proxmox web console is available at <code>https://&lt;host-ip>:8006</code>. For the sake of keeping these machines as pristine as possible, I&rsquo;m going to attempt to do the bare minimal configuration post-install and then use <a href=https://www.ansible.com/>Ansible</a> to further patches the hosts.</p><h2 id=next><a href=#next>#</a>
Next</h2><p>The base operating systems are configured, but right now they&rsquo;re independent systems. In the next part, I&rsquo;ll setup a two node Proxmox cluster with a quorum devices, setup ZFS pools for redundancy and some basic network storage.</p></div></article><div class=prev-next><div><a style=align-self:flex-end href=https://blog.reb.gg/posts/02-homelab-pt2/>Next &#8594;</a>
<span>üè° Homelab II: Proxmox cluster, ZFS and ‚Ä¶</span></div></div></main><footer><div class=stripes></div><div class=footer-meta><div><div>2024 Rob Herley</div><div>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> &
      <a href=https://github.com/robherley/rebugo target=_blank rel="noopener noreferrer">robherley/rebugo</a></div></div><div class=socials><a href=https://github.com/robherley rel=author title=github><i class="si si-github"></i></a><a href=https://x.com/robherley rel=author title=x><i class="si si-x"></i></a><a href=https://instagram.com/robherley rel=author title=instagram><i class="si si-instagram"></i></a><a href=https://www.linkedin.com/in/robherley/ rel=author title=linkedin><i class="si si-linkedin"></i></a></div></div></footer></body></html>