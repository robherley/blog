<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>üè° Homelab II: Proxmox cluster, ZFS and NFS | rob's blog</title><link rel=icon type=image/png href=https://blog.reb.gg/images/logo.png><meta property="og:url" content="https://blog.reb.gg/posts/02-homelab-pt2/">
<meta property="og:site_name" content="rob's blog"><meta property="og:title" content="üè° Homelab II: Proxmox cluster, ZFS and NFS"><meta property="og:description" content="In the previous part of this series, I assembled (and modified) hardware and setup the base operating systems on the machines. In this part, I‚Äôll go over how to connect the Proxmox nodes together, add a quorum device and provision some storage with ZFS. Also one of the ZFS drives will be configured with NFS to share container templates, ISOs, and snippets.
# Cluster If you only have one machine in your homelab, you can skip this step."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-10T00:00:00-05:00"><meta property="article:modified_time" content="2022-01-10T00:00:00-05:00"><meta property="article:tag" content="Proxmox"><meta property="article:tag" content="Zfs"><meta property="article:tag" content="Nfs"><meta property="article:tag" content="Cluster"><meta name=twitter:card content="summary"><meta name=twitter:title content="üè° Homelab II: Proxmox cluster, ZFS and NFS"><meta name=twitter:description content="In the previous part of this series, I assembled (and modified) hardware and setup the base operating systems on the machines. In this part, I‚Äôll go over how to connect the Proxmox nodes together, add a quorum device and provision some storage with ZFS. Also one of the ZFS drives will be configured with NFS to share container templates, ISOs, and snippets.
# Cluster If you only have one machine in your homelab, you can skip this step."><link rel=stylesheet href=/css/custom.min.faefcaabf97ea44e2b9626162349aeb966c6ea5166b9d744503b93cdd81173ad.css integrity="sha256-+u/Kq/l+pE4rliYWI0muuWbG6lFmuddEUDuTzdgRc60=" crossorigin=anonymous><link rel=stylesheet href=/css/article.min.3d68b337d54382250563c3c74528261d25b316f1df232ec198665d91e6621ba7.css integrity="sha256-PWizN9VDgiUFY8PHRSgmHSWzFvHfIy7BmGZdkeZiG6c=" crossorigin=anonymous><link rel=stylesheet href=/css/fonts.min.e0bf373169e09cf6dfc780f970638fcd39a5898dcecb6c9286f170d5907dbbf5.css integrity="sha256-4L83MWngnPbfx4D5cGOPzTmliY3Oy2yShvFw1ZB9u/U=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.61771936e5acd378a831a0ce3858ec58f08eeb0598159d8d3477988eb62c02ee.css integrity="sha256-YXcZNuWs03ioMaDOOFjsWPCO6wWYFZ2NNHeYjrYsAu4=" crossorigin=anonymous><link rel=stylesheet href=/css/simple-icons.min.min.8e53216f6540542c220ed40839d164fb18b299e5d70551cb60b0600d3f0a011f.css integrity="sha256-jlMhb2VAVCwiDtQIOdFk+xiymeXXBVHLYLBgDT8KAR8=" crossorigin=anonymous><style>:root{--color-primary:var(--color-blue-6)}</style><script src=/js/main.95176a6398aade31e73736833a061caca263c49d5f1e01d192b9e651423e0f8c.js integrity="sha256-lRdqY5iq3jHnNzaDOgYcrKJjxJ1fHgHRkrnmUUI+D4w=" crossorigin=anonymous></script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script><script defer data-domain=blog.reb.gg src=https://a.reb.gg/js/script.js></script></head><body><header><h1 class=site-title><span role=img aria-label=emoji-logo>üíæ
</span><a href=https://blog.reb.gg/>rob's blog</a></h1><nav><ul><li><a href=/>Home</a></li><li><a href=/posts>Posts</a></li></ul></nav></header><main><article><h1 class=title>üè° Homelab II: Proxmox cluster, ZFS and NFS</h1><div class=meta><time datetime=2022-01-10T00:00:00-05:00>January 10, 2022</time> &#183; 6 min read</div><div class=tags><a class=tag href=/tags/proxmox/>Proxmox</a>
<a class=tag href=/tags/zfs/>Zfs</a>
<a class=tag href=/tags/nfs/>Nfs</a>
<a class=tag href=/tags/cluster/>Cluster</a></div><div class=markdown-body><p>In the previous part of this series, I assembled (and modified) hardware and setup the base operating systems on the machines. In this part, I&rsquo;ll go over how to connect the Proxmox nodes together, add a quorum device and provision some storage with ZFS. Also one of the ZFS drives will be configured with NFS to share container templates, ISOs, and snippets.</p><h2 id=cluster><a href=#cluster>#</a>
Cluster</h2><p>If you only have one machine in your homelab, you can skip this step.</p><p>To create a cluster, pick one node to initialize it on:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>pvecm create rob-lab</span></span></code></pre></div><p>And now it&rsquo;s a one node Proxmox cluster.</p><p>Then <strong>on the second node to add</strong>, join via the <strong>first node&rsquo;s IP address</strong>:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@nuc$ </span>pvecm add 192.168.1.100</span></span></code></pre></div><p>Now it&rsquo;s a two-node cluster:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@nuc$ </span>pvecm status
</span></span><span style=display:flex><span>Cluster information
</span></span><span style=display:flex><span>-------------------
</span></span><span style=display:flex><span>Name:             rob-lab
</span></span><span style=display:flex><span>Config Version:   <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Transport:        knet
</span></span><span style=display:flex><span>Secure auth:      on
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Quorum information
</span></span><span style=display:flex><span>------------------
</span></span><span style=display:flex><span>Date:             Thu Dec <span style=color:#ef9f76>30</span> 20:22:25 <span style=color:#ef9f76>2021</span>
</span></span><span style=display:flex><span>Quorum provider:  corosync_votequorum
</span></span><span style=display:flex><span>Nodes:            <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Node ID:          0x00000002
</span></span><span style=display:flex><span>Ring ID:          1.9
</span></span><span style=display:flex><span>Quorate:          Yes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Votequorum information
</span></span><span style=display:flex><span>----------------------
</span></span><span style=display:flex><span>Expected votes:   <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Highest expected: <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Total votes:      <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Quorum:           <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Flags:            Quorate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Membership information
</span></span><span style=display:flex><span>----------------------
</span></span><span style=display:flex><span>    Nodeid      Votes Name
</span></span><span style=display:flex><span>0x00000001          <span style=color:#ef9f76>1</span> 192.168.1.100
</span></span><span style=display:flex><span>0x00000002          <span style=color:#ef9f76>1</span> 192.168.1.200 <span style=color:#99d1db;font-weight:700>(</span><span style=color:#99d1db>local</span><span style=color:#99d1db;font-weight:700>)</span></span></span></code></pre></div><p>Under the hood, Proxmox uses the <a href=https://github.com/corosync/corosync>corosync</a> cluster engine that uses a voting system with each node in the cluster. In an ideal scenario, there would be an odd number of nodes, but since I only have two machines I&rsquo;m going to setup the Raspberry Pi as a voter so that the cluster can properly reach quorum. The Pi is going to be configured as a corosync <a href=https://github.com/corosync/corosync-qdevice>qdevice</a>:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>pi@piprimary$ </span>sudo apt install corosync-qnetd corosync-qdevice</span></span></code></pre></div><p>Unfortunately the qdevice setup will require password auth via SSH to the root user. So the Pi&rsquo;s SSH configuration will temporarily be changed to allow root login via SSH and a root password must be set:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>pi@piprimary$ </span>sudo su -
</span></span><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@piprimary$ </span>passwd
</span></span><span style=display:flex><span>New password:
</span></span><span style=display:flex><span>Retype new password:
</span></span><span style=display:flex><span>passwd: password updated successfully
</span></span><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@piprimary$ </span>vi /etc/ssh/sshd_config <span style=color:#737994;font-style:italic># Set PermitRootLogin to &#34;yes&#34;</span>
</span></span><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@piprimary$ </span>systemctl restart sshd</span></span></code></pre></div><p>The qdevice package needs to be installed on each of the Proxmox nodes as well:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>apt install corosync-qdevice</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@nuc$ </span>apt install corosync-qdevice</span></span></code></pre></div><p>Adding the Pi as a qdevice to the cluster is slightly different from adding a normal node. On an already existing cluster node use <code>pvecm qdevice setup</code> to add the Pi by IP:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>pvecm qdevice setup 192.168.1.254</span></span></code></pre></div><p>Now, it&rsquo;s a two node cluster but with three members and three expected quorum votes:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>pvecm status
</span></span><span style=display:flex><span>Cluster information
</span></span><span style=display:flex><span>-------------------
</span></span><span style=display:flex><span>Name:             rob-lab
</span></span><span style=display:flex><span>Config Version:   <span style=color:#ef9f76>3</span>
</span></span><span style=display:flex><span>Transport:        knet
</span></span><span style=display:flex><span>Secure auth:      on
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Quorum information
</span></span><span style=display:flex><span>------------------
</span></span><span style=display:flex><span>Date:             Fri Dec <span style=color:#ef9f76>31</span> 12:41:02 <span style=color:#ef9f76>2021</span>
</span></span><span style=display:flex><span>Quorum provider:  corosync_votequorum
</span></span><span style=display:flex><span>Nodes:            <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Node ID:          0x00000001
</span></span><span style=display:flex><span>Ring ID:          1.9
</span></span><span style=display:flex><span>Quorate:          Yes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Votequorum information
</span></span><span style=display:flex><span>----------------------
</span></span><span style=display:flex><span>Expected votes:   <span style=color:#ef9f76>3</span>
</span></span><span style=display:flex><span>Highest expected: <span style=color:#ef9f76>3</span>
</span></span><span style=display:flex><span>Total votes:      <span style=color:#ef9f76>3</span>
</span></span><span style=display:flex><span>Quorum:           <span style=color:#ef9f76>2</span>
</span></span><span style=display:flex><span>Flags:            Quorate Qdevice
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Membership information
</span></span><span style=display:flex><span>----------------------
</span></span><span style=display:flex><span>    Nodeid      Votes    Qdevice Name
</span></span><span style=display:flex><span>0x00000001          <span style=color:#ef9f76>1</span>    A,V,NMW 192.168.1.100 <span style=color:#99d1db;font-weight:700>(</span><span style=color:#99d1db>local</span><span style=color:#99d1db;font-weight:700>)</span>
</span></span><span style=display:flex><span>0x00000002          <span style=color:#ef9f76>1</span>    A,V,NMW 192.168.1.200
</span></span><span style=display:flex><span>0x00000000          <span style=color:#ef9f76>1</span>            Qdevice</span></span></code></pre></div><p>Back on the Pi, disable SSH root login:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@piprimary$ </span>vi /etc/ssh/sshd_config <span style=color:#737994;font-style:italic># Set PermitRootLogin to &#34;no&#34;</span>
</span></span><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@piprimary$ </span>systemctl restart sshd</span></span></code></pre></div><h2 id=redundancy-with-zfs><a href=#redundancy-with-zfs>#</a>
Redundancy with ZFS</h2><p>While the NUC will just be using the single SSD for the host OS and all workload storage, the Poweredge has a few drives that need to be configured with ZFS.</p><p>ZFS filesystems are built on virtual storage pools. For now, there will be two pools, <code>ssd-mirror</code> and <code>rusty-z2</code>, as mentioned in the first post in this series. The third pool, <code>wolves-z</code> will be handled later, since the entire controller connecting the drives will be passed through to a VM.</p><p>Create a mirrored pool of two drives called <code>ssd-mirror</code>:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>zpool create ssd-mirror mirror /dev/sdo /dev/sdq</span></span></code></pre></div><p>Create a RAID-z2 pool of 11 drives called <code>rusty-z2</code> (the <code>/dev/</code> can be omitted):</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>zpool create rusty-z2 raidz2 sde sdf sdg sdh sdi sdj sdk sdl sdm sdn sdp</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>zfs list
</span></span><span style=display:flex><span>NAME         USED  AVAIL     REFER  MOUNTPOINT
</span></span><span style=display:flex><span>rusty-z2    1.10M  7.49T      219K  /rusty-z2
</span></span><span style=display:flex><span>ssd-mirror   528K   899G       96K  /ssd-mirror
</span></span><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>zpool status
</span></span><span style=display:flex><span>  pool: rusty-z2
</span></span><span style=display:flex><span> state: ONLINE
</span></span><span style=display:flex><span>config:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  NAME                           STATE     READ WRITE CKSUM
</span></span><span style=display:flex><span>  rusty-z2                       ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>    raidz2-0                     ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3QG5J  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3WGKZ  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3VHK5  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3TRW7  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      scsi-35000c50083a28083     ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      scsi-35000c50083a0395b     ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3WGCA  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3V6JB  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG40C6A  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG40JQH  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-ST91000640NS_9XG3VAEC  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>errors: No known data errors
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  pool: ssd-mirror
</span></span><span style=display:flex><span> state: ONLINE
</span></span><span style=display:flex><span>config:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  NAME                                  STATE     READ WRITE CKSUM
</span></span><span style=display:flex><span>  ssd-mirror                            ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>    mirror-0                            ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-CT1000MX500SSD1_2147E5E74EEA  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>      ata-CT1000MX500SSD1_2147E5E73F89  ONLINE       <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>     <span style=color:#ef9f76>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>errors: No known data errors</span></span></code></pre></div><p>To make these available to Proxmox, they&rsquo;ll need to be added manually to <code>/etc/pve/storage.cfg</code> like so:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>zfspool: rusty-z2
</span></span><span style=display:flex><span>  pool rusty-z2
</span></span><span style=display:flex><span>  content images,rootdir
</span></span><span style=display:flex><span>  mountpoint /rusty-z2
</span></span><span style=display:flex><span>  nodes r720
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>zfspool: ssd-mirror
</span></span><span style=display:flex><span>  pool ssd-mirror
</span></span><span style=display:flex><span>  content images,rootdir
</span></span><span style=display:flex><span>  mountpoint /ssd-mirror
</span></span><span style=display:flex><span>  nodes r720
</span></span></code></pre></div><p>Alternatively, this can be also done in Proxmox&rsquo;s web console under Node > Disks > ZFS > Create and it will create the zpool and storage entry all together. This is way easier than using the CLI but it&rsquo;s good to know how this is all happening behind the pretty web console.</p><p>Each <a href=https://pve.proxmox.com/wiki/Storage>storage type</a> in Proxmox has restrictions to the type of content it can hold. For instance, the <code>zfspool</code> type can only hold <code>images</code> or <code>rootdir</code>, which are VM disk images and container directories. For the <code>ssd-mirror</code> type this is perfect, because it will be for those exact workloads. For the <code>rusty-z2</code> pool, we&rsquo;ll need a different storage type.</p><p>To do so, initialize a new ZFS dataset called <code>pve</code> under <code>rusty-z2</code>:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>zfs create rusty-z2/pve</span></span></code></pre></div><p>Again, to use this in Proxmox it must be added to <code>/etc/pve/storage.cfg</code>:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>dir: rusty-dir
</span></span><span style=display:flex><span>  path /rusty-z2/pve
</span></span><span style=display:flex><span>  content backup,snippets,iso,vztmpl
</span></span><span style=display:flex><span>  nodes r720
</span></span><span style=display:flex><span>  prune-backups keep-all=1
</span></span></code></pre></div><p>Notice the <code>dir</code> type with a content of <code>backup,snippets,iso,vztmpl</code>. Once this is done, all of the storage will appear in the web console under the r720 node:</p><img src=/content/homelab/pve_r720_storage.png alt="r720 storage in proxmox console"><h2 id=zfs-shared-over-nfs><a href=#zfs-shared-over-nfs>#</a>
ZFS shared over NFS</h2><p>It would be really convient if the NUC could access the <code>rusty-dir</code> storage, so that it could use that redundant storage for backups and share ISOs, container templates, snippets, etc. With ZFS and NFS this is dead simple.</p><p>Add NFS server:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>apt install nfs-kernel-server</span></span></code></pre></div><p>Set the dataset to share NFS:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>zfs <span style=color:#99d1db>set</span> <span style=color:#f2d5cf>sharenfs</span><span style=color:#99d1db;font-weight:700>=</span><span style=color:#a6d189>&#39;rw&#39;</span> rusty-z2/pve</span></span></code></pre></div><p>On the NUC node, add the following to <code>/etc/pve/storage.cfg</code>:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext><span style=display:flex><span>nfs: rusty-nfs
</span></span><span style=display:flex><span>  export /rusty-z2/pve
</span></span><span style=display:flex><span>  path /mnt/pve/rusty-nfs
</span></span><span style=display:flex><span>  server 192.168.1.100
</span></span><span style=display:flex><span>  content backup,snippets,iso,vztmpl
</span></span><span style=display:flex><span>  nodes nuc
</span></span><span style=display:flex><span>  prune-backups keep-all=1
</span></span></code></pre></div><p>In the Proxmox console, the new NFS storage should appear under the NUC node:</p><img src=/content/homelab/pve_nuc_storage.png alt="nuc storage in proxmox console"><p>As a quick test, on the r720 node, download a container template:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@r720$ </span>pveam download rusty-dir ubuntu-20.04-standard_20.04-1_amd64.tar.gz</span></span></code></pre></div><p>And on the NUC node, it should appear in the corresponding NFS:</p><div class=highlight><pre tabindex=0 style=color:#c6d0f5;background-color:#303446;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#81c8be;user-select:none>root@nuc$ </span>pveam list rusty-nfs
</span></span><span style=display:flex><span>NAME                                                         SIZE
</span></span><span style=display:flex><span>rusty-nfs:vztmpl/ubuntu-20.04-standard_20.04-1_amd64.tar.gz  204.28MB</span></span></code></pre></div><p>Now the NUC can have redundant storage over NFS.</p><h2 id=next><a href=#next>#</a>
Next</h2><p>The machines are running, storage is configured and the cluster is ready for some workloads, but before that it&rsquo;d be a good idea to automate some of the preflight tasks. In the next part, I&rsquo;ll take a look at Ansible to harden access and handle any of the post-install configuration.</p></div></article><div class=prev-next><div><a href=https://blog.reb.gg/posts/01-homelab-pt1/>&#8592; Prev</a>
<span>üè° Homelab I: Intro, Hardware and Proxmox ‚Ä¶</span></div><div><a style=align-self:flex-end href=https://blog.reb.gg/posts/03-homelab-pt3/>Next &#8594;</a>
<span>üè° Homelab III: Automation with Ansible ‚Ä¶</span></div></div></main><footer><div class=stripes></div><div class=footer-meta><div><div>2024 Rob Herley</div><div>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> &
      <a href=https://github.com/robherley/rebugo target=_blank rel="noopener noreferrer">robherley/rebugo</a></div></div><div class=socials><a href=https://github.com/robherley rel=author title=github><i class="si si-github"></i></a><a href=https://x.com/robherley rel=author title=x><i class="si si-x"></i></a><a href=https://instagram.com/robherley rel=author title=instagram><i class="si si-instagram"></i></a><a href=https://www.linkedin.com/in/robherley/ rel=author title=linkedin><i class="si si-linkedin"></i></a></div></div></footer></body></html>