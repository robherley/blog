<!doctype html><html lang=en-us>
<head>
<link rel=preload href=/lib/font-awesome/webfonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/lib/font-awesome/webfonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/lib/font-awesome/webfonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous>
<link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous>
<script type=text/javascript src=https://latest.cactus.chat/cactus.js></script>
<link rel=stylesheet href=https://latest.cactus.chat/style.css type=text/css>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<title> Homelab Part II: Proxmox cluster, ZFS and NFS | rob's blog</title>
<link rel=canonical href=https://blog.reb.gg/posts/02-homelab-pt2/>
<meta name=description content="ðŸ‘‹ Hey y'all. This blog is my documented journey working with various aspects of technology. I'll try my best to keep things coherent and interesting, and hopefully I can teach a few people along the way.">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="all,follow">
<meta name=googlebot content="index,follow,snippet,archive">
<meta property="og:title" content="Homelab Part II: Proxmox cluster, ZFS and NFS">
<meta property="og:description" content="In the previous part of this series, I assembled (and modified) hardware and setup the base operating systems on the machines. In this part, I&rsquo;ll go over how to connect the Proxmox nodes together, add a quorum device and provision some storage with ZFS. Also one of the ZFS drives will be configured with NFS to share container templates, ISOs, and snippets.
Cluster If you only have one machine in your homelab, you can skip this step.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://blog.reb.gg/posts/02-homelab-pt2/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-01-10T00:00:00-05:00">
<meta property="article:modified_time" content="2022-01-10T00:00:00-05:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Homelab Part II: Proxmox cluster, ZFS and NFS">
<meta name=twitter:description content="In the previous part of this series, I assembled (and modified) hardware and setup the base operating systems on the machines. In this part, I&rsquo;ll go over how to connect the Proxmox nodes together, add a quorum device and provision some storage with ZFS. Also one of the ZFS drives will be configured with NFS to share container templates, ISOs, and snippets.
Cluster If you only have one machine in your homelab, you can skip this step.">
<link rel=stylesheet href=https://blog.reb.gg/css/styles.a20e9ff7e4966e5e04f7c5bb633c3e427d0e10d8d676b3c4b81da08966c34bd718f4bf0b40f3a4a8cb8495678b39d8007406db2549bc420e2c2e8086708b958c.css integrity="sha512-og6f9+SWbl4E98W7Yzw+Qn0OENjWdrPEuB2giWbDS9cY9L8LQPOkqMuElWeLOdgAdAbbJUm8Qg4sLoCGcIuVjA==">
<link rel=stylesheet href=https://blog.reb.gg/css/custom.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]-->
<link rel=icon type=image/png href=https://blog.reb.gg/images/favicon.ico>
</head>
<body class="max-width mx-auto px3 ltr">
<div class="content index py4">
<div id=header-post>
<a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick="$('html, body').animate({scrollTop:0},'fast')" style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu>
<span id=nav>
<ul>
<li><a href=/>Home</a></li>
<li><a href=/posts>Posts</a></li>
<li><a href=/tags>Tags</a></li>
</ul>
</span>
<br>
<span id=actions>
<ul>
<li>
<a class=icon href=https://blog.reb.gg/posts/01-homelab-pt1/ aria-label=Previous>
<i class="fas fa-chevron-left" aria-hidden=true onmouseover="$('#i-prev').toggle()" onmouseout="$('#i-prev').toggle()"></i>
</a>
</li>
<li>
<a class=icon href=https://blog.reb.gg/posts/03-homelab-pt3/ aria-label=Next>
<i class="fas fa-chevron-right" aria-hidden=true onmouseover="$('#i-next').toggle()" onmouseout="$('#i-next').toggle()"></i>
</a>
</li>
<li>
<a class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast')" aria-label="Top of Page">
<i class="fas fa-chevron-up" aria-hidden=true onmouseover="$('#i-top').toggle()" onmouseout="$('#i-top').toggle()"></i>
</a>
</li>
<li>
<a class=icon href=# aria-label=Share>
<i class="fas fa-share-alt" aria-hidden=true onmouseover="$('#i-share').toggle()" onmouseout="$('#i-share').toggle()" onclick="return $('#share').toggle(),!1"></i>
</a>
</li>
</ul>
<span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span>
</span>
<br>
<div id=share style=display:none>
<ul>
<li>
<a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f" aria-label=Facebook>
<i class="fab fa-facebook" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://twitter.com/share?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&text=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Twitter>
<i class="fab fa-twitter" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&title=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Linkedin>
<i class="fab fa-linkedin" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&is_video=false&description=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Pinterest>
<i class="fab fa-pinterest" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="mailto:?subject=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS&body=Check out this article: https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f" aria-label=Email>
<i class="fas fa-envelope" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&title=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Pocket>
<i class="fab fa-get-pocket" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&title=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=reddit>
<i class="fab fa-reddit" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&name=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS&description=In%20the%20previous%20part%20of%20this%20series%2c%20I%20assembled%20%28and%20modified%29%20hardware%20and%20setup%20the%20base%20operating%20systems%20on%20the%20machines.%20In%20this%20part%2c%20I%26rsquo%3bll%20go%20over%20how%20to%20connect%20the%20Proxmox%20nodes%20together%2c%20add%20a%20quorum%20device%20and%20provision%20some%20storage%20with%20ZFS.%20Also%20one%20of%20the%20ZFS%20drives%20will%20be%20configured%20with%20NFS%20to%20share%20container%20templates%2c%20ISOs%2c%20and%20snippets.%0aCluster%20If%20you%20only%20have%20one%20machine%20in%20your%20homelab%2c%20you%20can%20skip%20this%20step." aria-label=Tumblr>
<i class="fab fa-tumblr" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&t=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label="Hacker News">
<i class="fab fa-hacker-news" aria-hidden=true></i>
</a>
</li>
</ul>
</div>
</span>
</div>
<div style=margin-bottom:2rem>
<a href=https://blog.reb.gg/><i class="fas fa-angle-left"></i> Back</a>
</div>
<article class=post itemscope itemtype=http://schema.org/BlogPosting>
<header>
<h1 class=posttitle itemprop="name headline">
Homelab Part II: Proxmox cluster, ZFS and NFS
</h1>
<div class=meta>
<div class=postdate>
<time datetime="2022-01-10 00:00:00 -0500 -0500" itemprop=datePublished>2022 Jan 10</time>
</div>
<div class=article-read-time>
<i class="far fa-clock"></i>
5 minute read
</div>
<div class=article-tag>
<i class="fas fa-tag"></i>
<a class=tag-link href=/tags/proxmox rel=tag>proxmox</a>
,
<a class=tag-link href=/tags/zfs rel=tag>zfs</a>
,
<a class=tag-link href=/tags/nfs rel=tag>nfs</a>
,
<a class=tag-link href=/tags/cluster rel=tag>cluster</a>
</div>
</div>
</header>
<div id=toc>
<nav id=TableOfContents>
<ul>
<li><a href=#cluster>Cluster</a></li>
<li><a href=#redundancy-with-zfs>Redundancy with ZFS</a></li>
<li><a href=#zfs-shared-over-nfs>ZFS shared over NFS</a></li>
<li><a href=#next>Next</a></li>
</ul>
</nav>
</div>
<div class=content itemprop=articleBody>
<p>In the previous part of this series, I assembled (and modified) hardware and setup the base operating systems on the machines. In this part, I&rsquo;ll go over how to connect the Proxmox nodes together, add a quorum device and provision some storage with ZFS. Also one of the ZFS drives will be configured with NFS to share container templates, ISOs, and snippets.</p>
<h2 id=cluster>Cluster</h2>
<p>If you only have one machine in your homelab, you can skip this step.</p>
<p>To create a cluster, pick one node to initialize it on:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>pvecm create rob-lab</code></pre></div>
<p>And now it&rsquo;s a one node Proxmox cluster.</p>
<p>Then <strong>on the second node to add</strong>, join via the <strong>first node&rsquo;s IP address</strong>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@nuc$ </span>pvecm add 192.168.1.100</code></pre></div>
<p>Now it&rsquo;s a two-node cluster:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@nuc$ </span>pvecm status
Cluster information
-------------------
Name:             rob-lab
Config Version:   <span style=color:#bd93f9>2</span>
Transport:        knet
Secure auth:      on

Quorum information
------------------
Date:             Thu Dec <span style=color:#bd93f9>30</span> 20:22:25 <span style=color:#bd93f9>2021</span>
Quorum provider:  corosync_votequorum
Nodes:            <span style=color:#bd93f9>2</span>
Node ID:          0x00000002
Ring ID:          1.9
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   <span style=color:#bd93f9>2</span>
Highest expected: <span style=color:#bd93f9>2</span>
Total votes:      <span style=color:#bd93f9>2</span>
Quorum:           <span style=color:#bd93f9>2</span>
Flags:            Quorate

Membership information
----------------------
    Nodeid      Votes Name
0x00000001          <span style=color:#bd93f9>1</span> 192.168.1.100
0x00000002          <span style=color:#bd93f9>1</span> 192.168.1.200 <span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>local</span><span style=color:#ff79c6>)</span></code></pre></div>
<p>Under the hood, Proxmox uses the <a href=https://github.com/corosync/corosync>corosync</a> cluster engine that uses a voting system with each node in the cluster. In an ideal scenario, there would be an odd number of nodes, but since I only have two machines I&rsquo;m going to setup the Raspberry Pi as a voter so that the cluster can properly reach quorum. The Pi is going to be configured as a corosync <a href=https://github.com/corosync/corosync-qdevice>qdevice</a>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>pi@piprimary$ </span>sudo apt install corosync-qnetd corosync-qdevice</code></pre></div>
<p>Unfortunately the qdevice setup will require password auth via SSH to the root user. So the Pi&rsquo;s SSH configuration will temporarily be changed to allow root login via SSH and a root password must be set:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>pi@piprimary$ </span>sudo su -
<span style=color:#50fa7b;user-select:none>root@piprimary$ </span>passwd
New password:
Retype new password:
passwd: password updated successfully
<span style=color:#50fa7b;user-select:none>root@piprimary$ </span>vi /etc/ssh/sshd_config <span style=color:#6272a4># Set PermitRootLogin to &#34;yes&#34;</span>
<span style=color:#50fa7b;user-select:none>root@piprimary$ </span>systemctl restart sshd</code></pre></div>
<p>The qdevice package needs to be installed on each of the Proxmox nodes as well:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>apt install corosync-qdevice</code></pre></div>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@nuc$ </span>apt install corosync-qdevice</code></pre></div>
<p>Adding the Pi as a qdevice to the cluster is slightly different from adding a normal node. On an already existing cluster node use <code>pvecm qdevice setup</code> to add the Pi by IP:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>pvecm qdevice setup 192.168.1.254</code></pre></div>
<p>Now, it&rsquo;s a two node cluster but with three members and three expected quorum votes:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>pvecm status
Cluster information
-------------------
Name:             rob-lab
Config Version:   <span style=color:#bd93f9>3</span>
Transport:        knet
Secure auth:      on

Quorum information
------------------
Date:             Fri Dec <span style=color:#bd93f9>31</span> 12:41:02 <span style=color:#bd93f9>2021</span>
Quorum provider:  corosync_votequorum
Nodes:            <span style=color:#bd93f9>2</span>
Node ID:          0x00000001
Ring ID:          1.9
Quorate:          Yes

Votequorum information
----------------------
Expected votes:   <span style=color:#bd93f9>3</span>
Highest expected: <span style=color:#bd93f9>3</span>
Total votes:      <span style=color:#bd93f9>3</span>
Quorum:           <span style=color:#bd93f9>2</span>
Flags:            Quorate Qdevice

Membership information
----------------------
    Nodeid      Votes    Qdevice Name
0x00000001          <span style=color:#bd93f9>1</span>    A,V,NMW 192.168.1.100 <span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>local</span><span style=color:#ff79c6>)</span>
0x00000002          <span style=color:#bd93f9>1</span>    A,V,NMW 192.168.1.200
0x00000000          <span style=color:#bd93f9>1</span>            Qdevice</code></pre></div>
<p>Back on the Pi, disable SSH root login:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@piprimary$ </span>vi /etc/ssh/sshd_config <span style=color:#6272a4># Set PermitRootLogin to &#34;no&#34;</span>
<span style=color:#50fa7b;user-select:none>root@piprimary$ </span>systemctl restart sshd</code></pre></div>
<h2 id=redundancy-with-zfs>Redundancy with ZFS</h2>
<p>While the NUC will just be using the single SSD for the host OS and all workload storage, the Poweredge has a few drives that need to be configured with ZFS.</p>
<p>ZFS filesystems are built on virtual storage pools. For now, there will be two pools, <code>ssd-mirror</code> and <code>rusty-z2</code>, as mentioned in the first post in this series. The third pool, <code>wolves-z</code> will be handled later, since the entire controller connecting the drives will be passed through to a VM.</p>
<p>Create a mirrored pool of two drives called <code>ssd-mirror</code>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>zpool create ssd-mirror mirror /dev/sdo /dev/sdq</code></pre></div>
<p>Create a RAID-z2 pool of 11 drives called <code>rusty-z2</code> (the <code>/dev/</code> can be omitted):</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>zpool create rusty-z2 raidz2 sde sdf sdg sdh sdi sdj sdk sdl sdm sdn sdp</code></pre></div>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>zfs list
NAME         USED  AVAIL     REFER  MOUNTPOINT
rusty-z2    1.10M  7.49T      219K  /rusty-z2
ssd-mirror   528K   899G       96K  /ssd-mirror
<span style=color:#50fa7b;user-select:none>root@r720$ </span>zpool status
  pool: rusty-z2
 state: ONLINE
config:

  NAME                           STATE     READ WRITE CKSUM
  rusty-z2                       ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
    raidz2-0                     ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3QG5J  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3WGKZ  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3VHK5  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3TRW7  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      scsi-35000c50083a28083     ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      scsi-35000c50083a0395b     ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3WGCA  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3V6JB  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG40C6A  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG40JQH  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-ST91000640NS_9XG3VAEC  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>

errors: No known data errors

  pool: ssd-mirror
 state: ONLINE
config:

  NAME                                  STATE     READ WRITE CKSUM
  ssd-mirror                            ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
    mirror-0                            ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-CT1000MX500SSD1_2147E5E74EEA  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>
      ata-CT1000MX500SSD1_2147E5E73F89  ONLINE       <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span>

errors: No known data errors</code></pre></div>
<p>To make these available to Proxmox, they&rsquo;ll need to be added manually to <code>/etc/pve/storage.cfg</code> like so:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext>zfspool: rusty-z2
  pool rusty-z2
  content images,rootdir
  mountpoint /rusty-z2
  nodes r720

zfspool: ssd-mirror
  pool ssd-mirror
  content images,rootdir
  mountpoint /ssd-mirror
  nodes r720
</code></pre></div><p>Alternatively, this can be also done in Proxmox&rsquo;s web console under Node > Disks > ZFS > Create and it will create the zpool and storage entry all together. This is way easier than using the CLI but it&rsquo;s good to know how this is all happening behind the pretty web console.</p>
<p>Each <a href=https://pve.proxmox.com/wiki/Storage>storage type</a> in Proxmox has restrictions to the type of content it can hold. For instance, the <code>zfspool</code> type can only hold <code>images</code> or <code>rootdir</code>, which are VM disk images and container directories. For the <code>ssd-mirror</code> type this is perfect, because it will be for those exact workloads. For the <code>rusty-z2</code> pool, we&rsquo;ll need a different storage type.</p>
<p>To do so, initialize a new ZFS dataset called <code>pve</code> under <code>rusty-z2</code>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>zfs create rusty-z2/pve</code></pre></div>
<p>Again, to use this in Proxmox it must be added to <code>/etc/pve/storage.cfg</code>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext>dir: rusty-dir
  path /rusty-z2/pve
  content backup,snippets,iso,vztmpl
  nodes r720
  prune-backups keep-all=1
</code></pre></div><p>Notice the <code>dir</code> type with a content of <code>backup,snippets,iso,vztmpl</code>. Once this is done, all of the storage will appear in the web console under the r720 node:</p>
<p><img src=/homelab/pve_r720_storage.png alt="r720 storage in proxmox console"></p>
<h2 id=zfs-shared-over-nfs>ZFS shared over NFS</h2>
<p>It would be really convient if the NUC could access the <code>rusty-dir</code> storage, so that it could use that redundant storage for backups and share ISOs, container templates, snippets, etc. With ZFS and NFS this is dead simple.</p>
<p>Add NFS server:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>apt install nfs-kernel-server</code></pre></div>
<p>Set the dataset to share NFS:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>zfs <span style=color:#8be9fd;font-style:italic>set</span> <span style=color:#8be9fd;font-style:italic>sharenfs</span><span style=color:#ff79c6>=</span><span style=color:#f1fa8c>&#39;rw&#39;</span> rusty-z2/pve</code></pre></div>
<p>On the NUC node, add the following to <code>/etc/pve/storage.cfg</code>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-plaintext data-lang=plaintext>nfs: rusty-nfs
  export /rusty-z2/pve
  path /mnt/pve/rusty-nfs
  server 192.168.1.100
  content backup,snippets,iso,vztmpl
  nodes nuc
  prune-backups keep-all=1
</code></pre></div><p>In the Proxmox console, the new NFS storage should appear under the NUC node:</p>
<p><img src=/homelab/pve_nuc_storage.png alt="nuc storage in proxmox console"></p>
<p>As a quick test, on the r720 node, download a container template:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@r720$ </span>pveam download rusty-dir ubuntu-20.04-standard_20.04-1_amd64.tar.gz</code></pre></div>
<p>And on the NUC node, it should appear in the corresponding NFS:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#50fa7b;user-select:none>root@nuc$ </span>pveam list rusty-nfs
NAME                                                         SIZE
rusty-nfs:vztmpl/ubuntu-20.04-standard_20.04-1_amd64.tar.gz  204.28MB</code></pre></div>
<p>Now the NUC can have redundant storage over NFS.</p>
<h2 id=next>Next</h2>
<p>The machines are running, storage is configured and the cluster is ready for some workloads, but before that it&rsquo;d be a good idea to automate some of the preflight tasks. In the next part, I&rsquo;ll take a look at Ansible to harden access and handle any of the post-install configuration.</p>
</div>
</article>
<div style="margin:2rem 0">
<a style=float:left href=https://blog.reb.gg/posts/01-homelab-pt1/><i class="fas fa-angle-left"></i> Prev</a>
<a style=float:right href=https://blog.reb.gg/posts/03-homelab-pt3/>Next <i class="fas fa-angle-right"></i></a>
</div>
<div id=footer-post-container>
<div id=footer-post>
<div id=nav-footer style=display:none>
<ul>
<li><a href=/>Home</a></li>
<li><a href=/posts>Posts</a></li>
<li><a href=/tags>Tags</a></li>
</ul>
</div>
<div id=share-footer style=display:none>
<ul>
<li>
<a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f" aria-label=Facebook>
<i class="fab fa-facebook fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://twitter.com/share?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&text=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Twitter>
<i class="fab fa-twitter fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&title=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Linkedin>
<i class="fab fa-linkedin fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&is_video=false&description=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Pinterest>
<i class="fab fa-pinterest fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="mailto:?subject=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS&body=Check out this article: https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f" aria-label=Email>
<i class="fas fa-envelope fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&title=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=Pocket>
<i class="fab fa-get-pocket fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&title=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label=reddit>
<i class="fab fa-reddit fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&name=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS&description=In%20the%20previous%20part%20of%20this%20series%2c%20I%20assembled%20%28and%20modified%29%20hardware%20and%20setup%20the%20base%20operating%20systems%20on%20the%20machines.%20In%20this%20part%2c%20I%26rsquo%3bll%20go%20over%20how%20to%20connect%20the%20Proxmox%20nodes%20together%2c%20add%20a%20quorum%20device%20and%20provision%20some%20storage%20with%20ZFS.%20Also%20one%20of%20the%20ZFS%20drives%20will%20be%20configured%20with%20NFS%20to%20share%20container%20templates%2c%20ISOs%2c%20and%20snippets.%0aCluster%20If%20you%20only%20have%20one%20machine%20in%20your%20homelab%2c%20you%20can%20skip%20this%20step." aria-label=Tumblr>
<i class="fab fa-tumblr fa-lg" aria-hidden=true></i>
</a>
</li>
<li>
<a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fblog.reb.gg%2fposts%2f02-homelab-pt2%2f&t=Homelab%20Part%20II%3a%20Proxmox%20cluster%2c%20ZFS%20and%20NFS" aria-label="Hacker News">
<i class="fab fa-hacker-news fa-lg" aria-hidden=true></i>
</a>
</li>
</ul>
</div>
<div id=actions-footer>
<a id=menu-toggle class=icon href=# onclick="return $('#nav-footer').toggle(),!1" aria-label=Menu>
<i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=share-toggle class=icon href=# onclick="return $('#share-footer').toggle(),!1" aria-label=Share>
<i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick="$('html, body').animate({scrollTop:0},'fast')" aria-label="Top of Page">
<i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a>
</div>
</div>
</div>
<footer id=footer>
<div class=footer-left>
Copyright &copy; 2022 Rob Herley
</div>
<div class=footer-right>
<nav>
<ul>
<li><a href=/>Home</a></li>
<li><a href=/posts>Posts</a></li>
<li><a href=/tags>Tags</a></li>
</ul>
</nav>
</div>
</footer>
</div>
</body>
<link rel=stylesheet href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
</html>